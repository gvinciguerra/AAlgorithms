\section{Randomized min-cut algorithm}

Consider the randomized min-cut algorithm discussed in class. We have seen that its probability of success is at least $1/{n \choose 2}$, where $n$ is the number of its vertices.
\begin{itemize}
  \item Describe how to implement the algorithm when the graph is represented by adjacency lists, and analyze its running time. In particular, a contraction step can be done in $O(n)$ time.
  \item A weighted graph has a weight $w(e)$ on each edge $e$, which is a positive real number. The min-cut in this case is meant to be min-weighted cut, where the sum of the weights in the cut edges is minimum. Describe how to extend the algorithm to weighted graphs, and show that the probability of success is still $\geq 1/{n \choose 2}$ [hint: define the weighted degree of a node].
  \item Show that running the algorithm multiple times independently at random, and taking the minimum among the min-cuts thus produced, the probability of success can be made at least $1 - 1/n^c$ for a constant $c > 0$ (hence, with high probability).
\end{itemize}

\vspace{0.5cm}
\paragraph{Contraction step.} We assume that the adjacency lists $Adj[x]$ are sorted $\forall x \in V$. We also note that the multigraph is undirected, therefore $v\in Adj[u]$ if and only if $u\in Adj[v]$. We maintain an attribute $pe$ in each edge to store the number of parallel edges.

To contract an edge $(u, v)$, we have to merge the two adjacency lists $Adj[u]$ and $Adj[v]$ into a single \emph{sorted} adjacency list $Adj[uv]$ --- like the merge procedure in merge sort --- ensuring that:
\begin{itemize}
  \item if the current node in $Adj[u]$ is $v$, skip the node, since it will not appear in $Adj[uv]$ (do the same with $Adj[v]$ and $u$);
  \item if the current nodes are equal to $x$, add $x$ to $Adj[uv]$ and set $(uv, x).pe = (u,x).pe + (v,x).pe$.
\end{itemize}
Finally, we replace $Adj[u]$ and $Adj[v]$ with the newly created $Adj[uv]$. The total cost of this step is $O(n)$, as at most both lists containing $n$ elements each are scanned.

\paragraph{Weighted graph extension.} The min-weighted cut problem asks to find a cut $(S, \overline{S})$, that is, a partition of $V$ in two non empty subsets $S \subset V$ and $\overline{S}=V \setminus S$, that minimizes the sum of weights of the edges that cross the cut, formally $$\min \sum_{\substack{e=(u,v)\in E \\ u \in S, v \in \overline{S}}} \omega(e).$$

The Karger's algorithm for weighted graphs is
\begin{algorithmic}[1]
  \Function{Min-Cut-Size}{$G$}
    \While{$|V| > 2$}
	  \State Choose an edge $e$ at random with probability $\frac{\omega(e)}{\sum_{e' \in E} \omega(e')}$
	  \State $G \gets$ \Call{ContractEdge}{$G,e$}
	\EndWhile
	\State \Return $\omega(e)$ where $e$ is the edge that connects the two remaining nodes
  \EndFunction
\end{algorithmic}

\paragraph{Error probability.} We define the weighted degree of a vertex $v$ as sum of the edges' weights incident to it:
\begin{equation*}
\omega_{v} = \sum_{e\{v, \_\} \in E}\omega(e) + \sum_{e\{\_, v\} \in E}\omega(e)
\end{equation*}
We can then define the min cut as the partition whose weighted degrees sum is minimum:
\begin{equation*}
\omega_{min} = \sum_{e \in \text{min cut}}\omega(e)
\end{equation*}

Now, each vertex $v$ has weighted degree $\geq \omega_{min}$: if this were to be false a vertex with a degree $\omega_v < \omega_{min}$ would exist and be the min cut itself.
By hypothesis $\omega_{min}$ was the min cut, hence the contradiction.
This allows us to give a lower bound for the weighted degree of the entire graph $G$:
\begin{equation*}
\omega_G \geq \frac{n\omega_{min}}{2}
\end{equation*}
The algorithm chooses an edge $e$ with probability proportional to the edge's weight $\omega(e): \prob{e \text{ is chosen}} = \frac{\omega(e)}{\mathcal{\omega_G}}$.
Thus we have an error when one of the edges $e$ belonging to the min cut is chosen:
\begin{equation*}
\prob{\text{error}} = \frac{\omega_{min}}{\omega_G} \leq \frac{\omega_{min}}{\frac{n \cdot \omega_{min}}{2}} = \frac{2}{n}
\end{equation*}
on a single cut.
It follows trivially that we have a success probability of at least
\begin{equation*}
\prob{\text{success}} \geq 1 - \prob{\text{error}} = 1 - \frac{2}{n} = \frac{n - 2}{n}
\end{equation*}
On the next one, we are going to obtain the same error on a lesser graph with $n - 1$ vertexes and without the $e$ edge: the error probability is increased to $\frac{2}{n - 1}$ and consequently decreases the success probability: $1 - \Pr(error) = 1 - \frac{2}{n - 1} = \frac{n - 1 - 2}{n - 1}$.
This process is then repeated until $n = 2$.
As these events are independent one another we have a success probability of at least:
\begin{equation*}
\prod_{i = 2}^{n - 2} \frac{n - i}{n - i + 2} = {{n}\choose{2}}^{-1}
\end{equation*}

\paragraph{Probability of success after multiple executions.}
As previously stated we have an error probability of
\begin{equation*}
1 - \Pr({\text{success}}) = 1 - \frac{1}{{{n} \choose {2}}}
\end{equation*}
if we then run the algorithm some $d \cdot \frac{1}{{{n} \choose {2}}}$ times, the probability of success becomes
\begin{equation*}
1 - \left(1 - \frac{1}{{{n} \choose {2}}} \right)^{d \cdot \frac{1}{{{n} \choose {2}}}} \geq 1 - e^{d}
\end{equation*}
by $d = c \ln(n)$ we have an error probability of $\leq \frac{1}{n^c}$.