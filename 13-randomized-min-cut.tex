\section{Randomized min-cut algorithm}

Consider the randomized min-cut algorithm discussed in class. We have seen that its probability of success is at least $1/{n \choose 2}$, where $n$ is the number of its vertices.
\begin{itemize}
  \item Describe how to implement the algorithm when the graph is represented by adjacency lists, and analyze its running time. In particular, a contraction step can be done in $O(n)$ time.
  \item A weighted graph has a weight $w(e)$ on each edge $e$, which is a positive real number. The min-cut in this case is meant to be min-weighted cut, where the sum of the weights in the cut edges is minimum. Describe how to extend the algorithm to weighted graphs, and show that the probability of success is still $\geq 1/{n \choose 2}$ [hint: define the weighted degree of a node].
  \item Show that running the algorithm multiple times independently at random, and taking the minimum among the min-cuts thus produced, the probability of success can be made at least $1 - 1/n^c$ for a constant $c > 0$ (hence, with high probability).
\end{itemize}

\vspace{0.5cm}
\paragraph{Contraction step.} We assume that the adjacency lists $Adj[x]$ are sorted $\forall x \in V$. We also note that the multigraph is undirected, therefore $v\in Adj[u]$ if and only if $u\in Adj[v]$. We maintain an attribute $pe$ in each edge to store the number of parallel edges.

To contract an edge $(u, v)$, we have to merge the two adjacency lists $Adj[u]$ and $Adj[v]$ into a single \emph{sorted} adjacency list $Adj[uv]$ --- like the merge procedure in merge sort --- ensuring that:
\begin{itemize}
  \item if the current node in $Adj[u]$ is $v$, skip the node, since it will not appear in $Adj[uv]$ (do the same with $Adj[v]$ and $u$);
  \item if the current nodes are equal to $x$, add $x$ to $Adj[uv]$ and set $(uv, x).pe = (u,x).pe + (v,x).pe$.
\end{itemize}
Finally, we replace $Adj[u]$ and $Adj[v]$ with the newly created $Adj[uv]$. The total cost of this step is $O(n)$, as at most both lists containing $n$ elements each are scanned.

\paragraph{Weighted graph extension.} The min-weighted cut problem asks to find a cut $(S, \overline{S})$, that is, a partition of $V$ in two non empty subsets $S \subset V$ and $\overline{S}=V \setminus S$, that minimizes the sum of weights of the edges that cross the cut, formally $$\min \sum_{\substack{e=(u,v)\in E \\ u \in S, v \in \overline{S}}} \omega(e).$$

The Karger's algorithm for weighted graphs is
\begin{algorithmic}[1]
  \Function{Min-Cut-Size}{$G$}
    \While{$|V| > 2$}
	  \State Choose an edge $e$ at random with probability $\frac{\omega(e)}{\sum_{e' \in E} \omega(e')}$
	  \State $G \gets$ \Call{ContractEdge}{$G,e$}
	\EndWhile
	\State \Return $\omega(e)$ where $e$ is the edge that connects the two remaining nodes
  \EndFunction
\end{algorithmic}

\paragraph{Error probability.} We then have the weight for the overall graph: 
\begin{equation*}
\omega_G = \sum^{n}\left( \omega(n) \right)
\end{equation*}
and for the min-cut: $\omega_{min}$.
Similarly as we've seen in class we can define $\omega_G$ in function of $\omega_{min}$:
\begin{equation*}
\omega_G \geq \frac{\sum^{n} \left( \omega(n) \right)}{2} \geq \frac{n \cdot \omega_{min}}{2}
\end{equation*}

We associate to each edge a probability $\Pr(e) < 1: \sum_{e \in G.E} \Pr(e)= 1$ proportionate to its weight: the more the weight, the more the probability it's chosen, giving us an error probability of
\begin{equation*}
\Pr(error) = \frac{\Pr(e \in C \text{ is chosen})}{\Pr(e \in E \text{ is chosen })} = \frac{\textsc{weight(min-cut)}}{\textsc{weight(graph)}} = \frac{\omega_{min}}{\omega_G} = \frac{\omega_{min}}{\frac{n \cdot \omega_{min}}{2}} = \frac{2}{n}
\end{equation*}
That is, we reach the same error probability seen in class.
Therefore we can safely assume that the analysis is the same.

The algorithm does not need to be edited but in the computation of the min-cut, which is now $\omega_c$ and not $\sum^{e^{i, j}} \left( e_i \right)$ where $e_i$ is an edge between $i, j$ last remaining nodes in the cut.

\paragraph{Probability of success after multiple executions.}
By the analysis seen in class we have an error probability of
\begin{equation*}
1 - \Pr({\text{success}}) = 1 - \frac{1}{{{n} \choose {2}}}
\end{equation*}
if we then run the algorithm some $d \cdot \frac{1}{{{n} \choose {2}}}$ times, the probability of success becomes
\begin{equation*}
1 - \left(1 - \frac{1}{{{n} \choose {2}}} \right)^{d \cdot \frac{1}{{{n} \choose {2}}}} \geq 1 - e^{d}
\end{equation*}
by $d = c \ln(n)$ we have an error probability of $\leq \frac{1}{n^c}$.